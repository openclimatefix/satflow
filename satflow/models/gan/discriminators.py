import functools
import torch
from torch.nn.modules.pixelshuffle import PixelShuffle, PixelUnshuffle
from torch.nn.utils.parametrizations import spectral_norm
from torch import nn as nn
from torchvision.transforms import RandomCrop
from satflow.models.utils import get_conv_layer
from satflow.models.gan.common import get_norm_layer, init_net, LBlock, GBlock, DBlock
import antialiased_cnns


def define_discriminator(
    input_nc,
    ndf,
    netD,
    n_layers_D=3,
    norm="batch",
    init_type="normal",
    init_gain=0.02,
    conv_type: str = "standard",
):
    """Create a discriminator

    Parameters:
        input_nc (int)     -- the number of channels in input images
        ndf (int)          -- the number of filters in the first conv layer
        netD (str)         -- the architecture's name: basic | n_layers | pixel
        n_layers_D (int)   -- the number of conv layers in the discriminator; effective when netD=='n_layers'
        norm (str)         -- the type of normalization layers used in the network.
        init_type (str)    -- the name of the initialization method.
        init_gain (float)  -- scaling factor for normal, xavier and orthogonal.

    Returns a discriminator

    Our current implementation provides three types of discriminators:
        [basic]: 'PatchGAN' classifier described in the original pix2pix paper.
        It can classify whether 70Ã—70 overlapping patches are real or fake.
        Such a patch-level discriminator architecture has fewer parameters
        than a full-image discriminator and can work on arbitrarily-sized images
        in a fully convolutional fashion.

        [n_layers]: With this mode, you can specify the number of conv layers in the discriminator
        with the parameter <n_layers_D> (default=3 as used in [basic] (PatchGAN).)

        [pixel]: 1x1 PixelGAN discriminator can classify whether a pixel is real or not.
        It encourages greater color diversity but has no effect on spatial statistics.

    The discriminator has been initialized by <init_net>. It uses Leakly RELU for non-linearity.
    """
    net = None
    norm_layer = get_norm_layer(norm_type=norm)
    if netD == "basic":  # default PatchGAN classifier
        net = NLayerDiscriminator(
            input_nc, ndf, n_layers=3, norm_layer=norm_layer, conv_type=conv_type
        )
    elif netD == "n_layers":  # more options
        net = NLayerDiscriminator(
            input_nc, ndf, n_layers_D, norm_layer=norm_layer, conv_type=conv_type
        )
    elif netD == "pixel":  # classify if each pixel is real or fake
        net = PixelDiscriminator(input_nc, ndf, norm_layer=norm_layer, conv_type=conv_type)
    elif netD == "enhanced":
        net = CloudGANDiscriminator(
            input_channels=input_nc, num_filters=ndf, num_stages=3, conv_type=conv_type
        )
    else:
        raise NotImplementedError("Discriminator model name [%s] is not recognized" % netD)
    return init_net(net, init_type, init_gain)


class GANLoss(nn.Module):
    """Define different GAN objectives.

    The GANLoss class abstracts away the need to create the target label tensor
    that has the same size as the input.
    """

    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):
        """Initialize the GANLoss class.

        Parameters:
            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.
            target_real_label (bool) - - label for a real image
            target_fake_label (bool) - - label of a fake image

        Note: Do not use sigmoid as the last layer of Discriminator.
        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.
        """
        super(GANLoss, self).__init__()
        self.register_buffer("real_label", torch.tensor(target_real_label))
        self.register_buffer("fake_label", torch.tensor(target_fake_label))
        self.gan_mode = gan_mode
        if gan_mode == "lsgan":
            self.loss = nn.MSELoss()
        elif gan_mode == "vanilla":
            self.loss = nn.BCEWithLogitsLoss()
        elif gan_mode in ["wgangp"]:
            self.loss = None
        else:
            raise NotImplementedError("gan mode %s not implemented" % gan_mode)

    def get_target_tensor(self, prediction, target_is_real):
        """Create label tensors with the same size as the input.

        Parameters:
            prediction (tensor) - - tpyically the prediction from a discriminator
            target_is_real (bool) - - if the ground truth label is for real images or fake images

        Returns:
            A label tensor filled with ground truth label, and with the size of the input
        """

        if target_is_real:
            target_tensor = self.real_label
        else:
            target_tensor = self.fake_label
        return target_tensor.expand_as(prediction)

    def __call__(self, prediction, target_is_real):
        """Calculate loss given Discriminator's output and grount truth labels.

        Parameters:
            prediction (tensor) - - tpyically the prediction output from a discriminator
            target_is_real (bool) - - if the ground truth label is for real images or fake images

        Returns:
            the calculated loss.
        """
        if self.gan_mode in ["lsgan", "vanilla"]:
            target_tensor = self.get_target_tensor(prediction, target_is_real)
            loss = self.loss(prediction, target_tensor)
        elif self.gan_mode == "wgangp":
            if target_is_real:  # Its real
                loss = -prediction.mean()
            else:
                loss = prediction.mean()
        return loss


class NLayerDiscriminator(nn.Module):
    """Defines a PatchGAN discriminator"""

    def __init__(
        self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, conv_type: str = "standard"
    ):
        """Construct a PatchGAN discriminator

        Parameters:
            input_nc (int)  -- the number of channels in input images
            ndf (int)       -- the number of filters in the last conv layer
            n_layers (int)  -- the number of conv layers in the discriminator
            norm_layer      -- normalization layer
        """
        super(NLayerDiscriminator, self).__init__()
        if (
            type(norm_layer) == functools.partial
        ):  # no need to use bias as BatchNorm2d has affine parameters
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        conv2d = get_conv_layer(conv_type)

        kw = 4
        padw = 1
        sequence = [
            conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),
            nn.LeakyReLU(0.2, True),
        ]
        nf_mult = 1
        nf_mult_prev = 1
        for n in range(1, n_layers):  # gradually increase the number of filters
            nf_mult_prev = nf_mult
            nf_mult = min(2 ** n, 8)
            if conv_type == "antialiased":
                block = [
                    conv2d(
                        ndf * nf_mult_prev,
                        ndf * nf_mult,
                        kernel_size=kw,
                        stride=1,
                        padding=padw,
                        bias=use_bias,
                    ),
                    norm_layer(ndf * nf_mult),
                    nn.LeakyReLU(0.2, True),
                    antialiased_cnns.BlurPool(ndf * nf_mult, stride=2),
                ]
            else:
                block = [
                    conv2d(
                        ndf * nf_mult_prev,
                        ndf * nf_mult,
                        kernel_size=kw,
                        stride=2,
                        padding=padw,
                        bias=use_bias,
                    ),
                    norm_layer(ndf * nf_mult),
                    nn.LeakyReLU(0.2, True),
                ]
            sequence += block

        nf_mult_prev = nf_mult
        nf_mult = min(2 ** n_layers, 8)
        sequence += [
            conv2d(
                ndf * nf_mult_prev,
                ndf * nf_mult,
                kernel_size=kw,
                stride=1,
                padding=padw,
                bias=use_bias,
            ),
            norm_layer(ndf * nf_mult),
            nn.LeakyReLU(0.2, True),
        ]

        sequence += [
            conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)
        ]  # output 1 channel prediction map
        self.model = nn.Sequential(*sequence)

    def forward(self, input):
        """Standard forward."""
        return self.model(input)


class PixelDiscriminator(nn.Module):
    """Defines a 1x1 PatchGAN discriminator (pixelGAN)"""

    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d, conv_type: str = "standard"):
        """Construct a 1x1 PatchGAN discriminator

        Parameters:
            input_nc (int)  -- the number of channels in input images
            ndf (int)       -- the number of filters in the last conv layer
            norm_layer      -- normalization layer
        """
        super(PixelDiscriminator, self).__init__()
        if (
            type(norm_layer) == functools.partial
        ):  # no need to use bias as BatchNorm2d has affine parameters
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        conv2d = get_conv_layer(conv_type)

        self.net = [
            conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),
            nn.LeakyReLU(0.2, True),
            conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),
            norm_layer(ndf * 2),
            nn.LeakyReLU(0.2, True),
            conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias),
        ]

        self.net = nn.Sequential(*self.net)

    def forward(self, input):
        """Standard forward."""
        return self.net(input)


class CloudGANBlock(nn.Module):
    def __init__(self, input_channels, conv_type: str = "standard"):
        super().__init__()
        conv2d = get_conv_layer(conv_type)
        self.conv = conv2d(input_channels, input_channels * 2, kernel_size=(3, 3))
        self.relu = torch.nn.ReLU()
        if conv_type == "antialiased":
            self.pool = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=1)
            self.blurpool = antialiased_cnns.BlurPool(input_channels * 2, stride=2)
        else:
            self.pool = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=2)
            self.blurpool = torch.nn.Identity()

    def forward(self, x):
        x = self.conv(x)
        x = self.relu(x)
        x = self.pool(x)
        x = self.blurpool(x)
        return x


class CloudGANDiscriminator(nn.Module):
    """Defines a discriminator based off https://www.climatechange.ai/papers/icml2021/54/slides.pdf"""

    def __init__(
        self,
        input_channels: int = 12,
        num_filters: int = 64,
        num_stages: int = 3,
        conv_type: str = "standard",
    ):
        super().__init__()
        conv2d = get_conv_layer(conv_type)
        self.conv_1 = conv2d(input_channels, num_filters, kernel_size=1, stride=1, padding=0)
        self.stages = []
        for stage in range(num_stages):
            self.stages.append(CloudGANBlock(num_filters, conv_type))
            num_filters = num_filters * 2
        self.stages = torch.nn.Sequential(*self.stages)
        self.flatten = torch.nn.Flatten()
        self.fc = torch.nn.LazyLinear(1)  # Real/Fake

    def forward(self, x):
        x = self.conv_1(x)
        x = self.stages(x)
        x = self.flatten(x)
        x = self.fc(x)
        return x


class NowcastingTemporalDiscriminator(torch.nn.Module):
    def __init__(self, input_channels: int = 12, crop_size: int = 128):
        super().__init__()
        self.transform = RandomCrop(crop_size)
        self.space2depth = PixelUnshuffle(downscale_factor=2)
        self.d1 = DBlock(
            input_channels=input_channels, output_channels=48, conv_type="3d", first_relu=False
        )
        self.d2 = DBlock(input_channels=48, output_channels=96, conv_type="3d")
        self.d3 = DBlock(input_channels=96, output_channels=192)
        self.d4 = DBlock(input_channels=192, output_channels=384)
        self.d5 = DBlock(input_channels=384, output_channels=768)
        self.d6 = DBlock(input_channels=768, output_channels=768, keep_same_output=True)

        self.fc = spectral_norm(torch.nn.Linear(768, 2))

    def forward(self, x):
        x = self.transform(x)
        x = self.space2depth(x)
        x = self.d1(x)
        x = self.d2(x)
        x = self.d3(x)
        x = self.d4(x)
        x = self.d5(x)
        x = self.d6(x)
        x = torch.sum(x.view(x.size(0), x.size(1), -1), dim=2)  # Sum pool
        x = self.fc(x)
        return torch.nn.ReLU(x)


class NowcastingSpatialDiscriminator(torch.nn.Module):
    def __init__(
        self,
        input_channels: int = 12,
        num_timesteps: int = 8,
    ):
        super().__init__()
        # Randomly, uniformly, select 8 timesteps to do this on from the input
        self.num_timesteps = num_timesteps
        # First step is mean pooling 2x2 to reduce input by half
        self.mean_pool = torch.nn.AvgPool2d(2)
        self.space2depth = PixelUnshuffle(downscale_factor=2)
        self.d1 = DBlock(input_channels=input_channels, output_channels=48, first_relu=False)
        self.d2 = DBlock(input_channels=48, output_channels=96)
        self.d3 = DBlock(input_channels=96, output_channels=192)
        self.d4 = DBlock(input_channels=192, output_channels=384)
        self.d5 = DBlock(input_channels=384, output_channels=768)
        self.d6 = DBlock(input_channels=768, output_channels=768, keep_same_output=True)

        # Spectrally normalized linear layer for binary classification
        self.fc = spectral_norm(torch.nn.Linear(768, 2))

    def forward(self, x):
        # x should be the chosen 8 or so
        idxs = torch.randint(low=0, high=x.size(1), size=(self.num_timesteps,))
        representations = []
        for idx in idxs:
            rep = self.mean_pool(x[:, idx, :, :, :])  # 128x128
            rep = self.space2depth(rep)  # 64x64x4
            rep = self.d1(rep)  # 32x32
            rep = self.d2(rep)  # 16x16
            rep = self.d3(rep)  # 8x8
            rep = self.d4(rep)  # 4x4
            rep = self.d5(rep)  # 2x2
            rep = self.d6(rep)  # 2x2

            # Sum-pool along width and height all 8 representations, pretty sure only the last output
            rep = torch.sum(rep.view(rep.size(0), rep.size(1), -1), dim=2)
            rep = self.fc(rep)
            representations.append(rep)

        # The representations are summed together before the ReLU
        x = torch.stack(representations, dim=0).sum(dim=0)  # Should be right shape? TODO Check
        # ReLU the output
        x = torch.nn.ReLU(x)
        return x
